{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-8d19745efc70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Visualization library based on matplotlib, provides interface for drawing attractive statistical graphics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Collection of functions for scientific and publication-ready visualization\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Visualization library based on matplotlib, provides interface for drawing attractive statistical graphics\n",
    "# import seaborn as sns  # Install seaborn in myEnvPy3 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the split data\n",
    "splitData = pd.read_csv('./data/splitKinectData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To describe the data with count, mean, std, min, etc..\n",
    "\n",
    "# splitData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the top part of data\n",
    "\n",
    "# splitData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the bottom part of the data\n",
    "\n",
    "# splitData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see a sample of whole data\n",
    "\n",
    "# splitData.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check datatype and other info of each column\n",
    "# By lookin at total number of entries and number of non-null objects we can check \n",
    "# if there are any null values or missing data\n",
    "\n",
    "# splitData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping 'face_engaged': no = 0; yes = 1\n",
    "# With yes and no: face_looking away, face_engaged\n",
    "\n",
    "yesNoList = ['face_engaged', 'face_lookingaway']\n",
    "\n",
    "for i in range(len(yesNoList)):\n",
    "    splitData[yesNoList[i]] = splitData[yesNoList[i]].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)\n",
    "    \n",
    "# Mapping 'face_glasses': unknown & no = 0; yes = 1 ###Commented and created a loop###\n",
    "# splitData['face_glasses'] = splitData['face_glasses'].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)\n",
    "\n",
    "# unknown, no yes: face_happy, face_lefteyeclosed, face_mouthmoved, face_mouthopen, face_righteyeclosed\n",
    "unknownYesNoList = ['face_glasses', 'face_happy', 'face_lefteyeclosed', 'face_mouthmoved',\n",
    "                    'face_mouthopen', 'face_righteyeclosed']\n",
    "for i in range(len(unknownYesNoList)):\n",
    "    splitData[unknownYesNoList[i]] = splitData[unknownYesNoList[i]].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping face size since it's a column of zeros\n",
    "# splitData.drop(columns=['face_size'])\n",
    "# splitData\n",
    "# splitData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping First column of indexes since it is redundant\n",
    "# splitData.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# splitData.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing individual features\n",
    "# splitData['pedes_posX'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing the data to a unsupervised learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling features to a range\n",
    "# initialize Min-max scaler\n",
    "minMaxScaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# copy df to be compared later\n",
    "unprocessData = splitData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Data\n",
    "# dataNormalized = preprocessing.normalize(unprocessData, norm='l2')\n",
    "\n",
    "# MinMax Scaling Unprocessed Data\n",
    "dataMinMax = minMaxScaler.fit_transform(unprocessData)\n",
    "\n",
    "# MinMax Scaling Normalized Data\n",
    "# dataMinMax = minMaxScaler.fit_transform(dataNormalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Check if k-value for PCA retains 99% of variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8834624645663419"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Covariance Matrix\n",
    "numTrainEg = dataMinMax.shape\n",
    "covarMatrix = (1/numTrainEg[0])*(np.matmul(dataMinMax.T, dataMinMax))\n",
    "covarMatrix.shape\n",
    "# Get svd of Covariance Matrix\n",
    "u, s, v = np.linalg.svd(covarMatrix, full_matrices=True)\n",
    "# Checking how much variance we have retained\n",
    "sum(s[:2])/sum(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try k-means clustering\n",
    "# import clusters\n",
    "from sklearn import cluster\n",
    "\n",
    "# initialize k-means\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(dataMinMax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = k_means.labels_\n",
    "# centers = k_means.cluster_centers_\n",
    "# help(k_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-191928ec77d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkmeansReduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# reduced_data = PCA(n_components=2).fit_transform(dataNormalized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "kmeansReduced = cluster.KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "\n",
    "# reduced_data = PCA(n_components=2).fit_transform(dataNormalized)\n",
    "reduced_data = PCA(n_components=2).fit_transform(dataMinMax)\n",
    "\n",
    "kmeansReduced.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeansReduced.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeansReduced.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "# plt.title('K-means clustering on Normalized Data\\n'\n",
    "#           'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# Make a selection\n",
    "# plt.savefig('kMeansMinMaxPCA.eps', format='eps', dpi=100)\n",
    "# plt.savefig('kMeansNormalizedPCA.eps', format='eps', dpi=100)\n",
    "# plt.savefig('kMeansNormMinMaxPCA.eps', format='eps', dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3994836  0.58945631]\n"
     ]
    }
   ],
   "source": [
    "# Only Testing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# kmeansReduced = cluster.KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "\n",
    "# reduced_data = PCA(n_components=2).fit_transform(dataNormalized)\n",
    "reduced_data = PCA(n_components=2)\n",
    "reduced_data.fit_transform(dataMinMax)\n",
    "print(reduced_data.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomoyaData = pd.read_csv('tomoyoLeapCat_100_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>thumbProximal_L_X</th>\n",
       "      <th>thumbProximal_L_Y</th>\n",
       "      <th>thumbProximal_L_Z</th>\n",
       "      <th>thumbDistal_L_X</th>\n",
       "      <th>thumbDistal_L_Y</th>\n",
       "      <th>thumbDistal_L_Z</th>\n",
       "      <th>thumbEF_L_X</th>\n",
       "      <th>thumbEF_L_Y</th>\n",
       "      <th>...</th>\n",
       "      <th>armFrontRadius_R_Z</th>\n",
       "      <th>armFrontUnla_R_X</th>\n",
       "      <th>armFrontUnla_R_Y</th>\n",
       "      <th>armFrontUnla_R_Z</th>\n",
       "      <th>armBackLateral_R_X</th>\n",
       "      <th>armBackLateral_R_Y</th>\n",
       "      <th>armBackLateral_R_Z</th>\n",
       "      <th>armBackMedial_R_X</th>\n",
       "      <th>armBackMedial_R_Y</th>\n",
       "      <th>armBackMedial_R_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.154221</td>\n",
       "      <td>-0.009801</td>\n",
       "      <td>0.351608</td>\n",
       "      <td>-0.154221</td>\n",
       "      <td>-0.009801</td>\n",
       "      <td>0.351608</td>\n",
       "      <td>-0.125315</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320950</td>\n",
       "      <td>0.071754</td>\n",
       "      <td>-0.031825</td>\n",
       "      <td>0.354184</td>\n",
       "      <td>0.191447</td>\n",
       "      <td>-0.170060</td>\n",
       "      <td>0.207293</td>\n",
       "      <td>0.208151</td>\n",
       "      <td>-0.180164</td>\n",
       "      <td>0.240527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>-0.154094</td>\n",
       "      <td>-0.009933</td>\n",
       "      <td>0.351446</td>\n",
       "      <td>-0.154094</td>\n",
       "      <td>-0.009933</td>\n",
       "      <td>0.351446</td>\n",
       "      <td>-0.125453</td>\n",
       "      <td>0.021754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307211</td>\n",
       "      <td>0.077601</td>\n",
       "      <td>-0.025145</td>\n",
       "      <td>0.341086</td>\n",
       "      <td>0.193232</td>\n",
       "      <td>-0.174480</td>\n",
       "      <td>0.202013</td>\n",
       "      <td>0.209081</td>\n",
       "      <td>-0.183807</td>\n",
       "      <td>0.235887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>-0.154779</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>0.351216</td>\n",
       "      <td>-0.154779</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>0.351216</td>\n",
       "      <td>-0.126826</td>\n",
       "      <td>0.022690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296780</td>\n",
       "      <td>0.208874</td>\n",
       "      <td>-0.014308</td>\n",
       "      <td>0.330060</td>\n",
       "      <td>0.160117</td>\n",
       "      <td>-0.214562</td>\n",
       "      <td>0.218106</td>\n",
       "      <td>0.173198</td>\n",
       "      <td>-0.228937</td>\n",
       "      <td>0.251386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>-0.154391</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>0.350695</td>\n",
       "      <td>-0.154391</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>0.350695</td>\n",
       "      <td>-0.126037</td>\n",
       "      <td>0.022484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290330</td>\n",
       "      <td>0.178661</td>\n",
       "      <td>-0.004194</td>\n",
       "      <td>0.325022</td>\n",
       "      <td>0.171782</td>\n",
       "      <td>-0.210765</td>\n",
       "      <td>0.214251</td>\n",
       "      <td>0.183721</td>\n",
       "      <td>-0.222570</td>\n",
       "      <td>0.248943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>-0.154457</td>\n",
       "      <td>-0.009486</td>\n",
       "      <td>0.351018</td>\n",
       "      <td>-0.154457</td>\n",
       "      <td>-0.009486</td>\n",
       "      <td>0.351018</td>\n",
       "      <td>-0.126409</td>\n",
       "      <td>0.022589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295726</td>\n",
       "      <td>0.199935</td>\n",
       "      <td>-0.009963</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.164159</td>\n",
       "      <td>-0.211359</td>\n",
       "      <td>0.215317</td>\n",
       "      <td>0.174564</td>\n",
       "      <td>-0.225407</td>\n",
       "      <td>0.249667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Time   thumbProximal_L_X   thumbProximal_L_Y  \\\n",
       "13          13    13           -0.154221           -0.009801   \n",
       "31          31    31           -0.154094           -0.009933   \n",
       "64          64    64           -0.154779           -0.009379   \n",
       "77          77    77           -0.154391           -0.009295   \n",
       "70          70    70           -0.154457           -0.009486   \n",
       "\n",
       "     thumbProximal_L_Z   thumbDistal_L_X   thumbDistal_L_Y   thumbDistal_L_Z  \\\n",
       "13            0.351608         -0.154221         -0.009801          0.351608   \n",
       "31            0.351446         -0.154094         -0.009933          0.351446   \n",
       "64            0.351216         -0.154779         -0.009379          0.351216   \n",
       "77            0.350695         -0.154391         -0.009295          0.350695   \n",
       "70            0.351018         -0.154457         -0.009486          0.351018   \n",
       "\n",
       "     thumbEF_L_X   thumbEF_L_Y         ...           armFrontRadius_R_Z  \\\n",
       "13     -0.125315      0.021736         ...                     0.320950   \n",
       "31     -0.125453      0.021754         ...                     0.307211   \n",
       "64     -0.126826      0.022690         ...                     0.296780   \n",
       "77     -0.126037      0.022484         ...                     0.290330   \n",
       "70     -0.126409      0.022589         ...                     0.295726   \n",
       "\n",
       "     armFrontUnla_R_X   armFrontUnla_R_Y   armFrontUnla_R_Z  \\\n",
       "13           0.071754          -0.031825           0.354184   \n",
       "31           0.077601          -0.025145           0.341086   \n",
       "64           0.208874          -0.014308           0.330060   \n",
       "77           0.178661          -0.004194           0.325022   \n",
       "70           0.199935          -0.009963           0.330075   \n",
       "\n",
       "     armBackLateral_R_X   armBackLateral_R_Y   armBackLateral_R_Z  \\\n",
       "13             0.191447            -0.170060             0.207293   \n",
       "31             0.193232            -0.174480             0.202013   \n",
       "64             0.160117            -0.214562             0.218106   \n",
       "77             0.171782            -0.210765             0.214251   \n",
       "70             0.164159            -0.211359             0.215317   \n",
       "\n",
       "     armBackMedial_R_X   armBackMedial_R_Y   armBackMedial_R_Z  \n",
       "13            0.208151           -0.180164            0.240527  \n",
       "31            0.209081           -0.183807            0.235887  \n",
       "64            0.173198           -0.228937            0.251386  \n",
       "77            0.183721           -0.222570            0.248943  \n",
       "70            0.174564           -0.225407            0.249667  \n",
       "\n",
       "[5 rows x 164 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tomoyaData.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio Data Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop last n rows\n",
    "n = 100\n",
    "splitData.drop(splitData.tail(n).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate into three new df\n",
    "approachDf = splitData.iloc[:300, :]\n",
    "interactDf = splitData.iloc[300:600, :]\n",
    "leaveDf = splitData.iloc[600:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "approachDf.to_csv('approachDf' + '.csv')\n",
    "interactDf.to_csv('interactDf' + '.csv')\n",
    "leaveDf.to_csv('leaveDf' + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomoya part\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from keras.utils import to_categorical\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_2_one_hots(labels=''):\n",
    "    words = []\n",
    "    new_word_id = 0\n",
    "    dictionary = {}\n",
    "    labels = labels[:, np.newaxis]\n",
    "    # sort by alphabetical order\n",
    "    labelsSorted = np.sort(labels, axis=0)\n",
    "\n",
    "    for word in labelsSorted:\n",
    "        if word[0] not in dictionary:\n",
    "            dictionary[word[0]] = new_word_id\n",
    "            new_word_id += 1\n",
    "\n",
    "\n",
    "            for word in labels:\n",
    "        words.append(dictionary[word[0]])\n",
    "\n",
    "    one_hots = to_categorical(words)\n",
    "    dictionary_inv = {dictionary[k]: k for k in dictionary}\n",
    "\n",
    "    return dictionary, one_hots, dictionary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data():\n",
    "\n",
    "    # Get current working directory of process\n",
    "    cwd = os.getcwd()\n",
    "    path = cwd + '/data' + '/deepLearning'\n",
    "    files = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            files.append(file)\n",
    "\n",
    "    files = np.array(files)\n",
    "    labelsOutput = []\n",
    "    InputAll = []\n",
    "\n",
    "    for i in range(0, int(files.shape[0])):\n",
    "        dfInput = pd.read_csv(path + '/' + files[i], sep=',')\n",
    "    #     dfInput.dropna(how='any') # Drop missing values\n",
    "        dfInput.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "        print(files[i] + ' open')\n",
    "        label = str.lower(files[i][:-4]) # Remove '.csv'\n",
    "        InputNames = dfInput.columns\n",
    "\n",
    "        Input = dfInput.as_matrix() # Convert the frame to its Numpy-array representation\n",
    "        nCut = int(Input.shape[0] / 100)  \n",
    "\n",
    "        # Convert from Rows, Columns(2D) to nCut, 100, Columns(3D)\n",
    "        Input = np.array(np.split(Input, nCut, axis=0))\n",
    "        # Transpose to 100, Columns, nCut\n",
    "        Input = Input.transpose([1, 2, 0])\n",
    "\n",
    "        labelsOutput.append([label for j in range(0, nCut)])\n",
    "\n",
    "        if InputAll == []:\n",
    "            InputAll = Input\n",
    "        else:\n",
    "            InputAll = np.concatenate([InputAll, Input], axis=2)\n",
    "\n",
    "    labelsOutput = np.array(list(chain.from_iterable(labelsOutput))) # chain('ABC', 'DEF') --> A B C D E F\n",
    "\n",
    "    p = np.random.permutation(InputAll.shape[2])\n",
    "\n",
    "    InputAll = InputAll[:, :, p]\n",
    "\n",
    "    labelsOutput = labelsOutput[p]\n",
    "\n",
    "    dictionary, one_hots, dictionary_inv = labels_2_one_hots(labelsOutput)\n",
    "    \n",
    "    return InputAll, dictionary, one_hots, InputNames, dictionary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(data):\n",
    "    std = []\n",
    "    mean = []\n",
    "    dataOut = data.copy()\n",
    "\n",
    "    std.append(np.std(data, ddof=1))\n",
    "    mean.append(np.mean(data))\n",
    "    dataOut = (data - mean) / std\n",
    "\n",
    "    dataOut[np.isnan(dataOut)] = 0\n",
    "\n",
    "    return dataOut, mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaveDf.csv open\n",
      "approachDf.csv open\n",
      "interactDf.csv open\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/envs/myEnvPy3/lib/python3.6/site-packages/ipykernel/__main__.py:33: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "InputAll, dictionary, one_hots, InputNames, dictionary_inv = get_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(RNN):\n",
    "\n",
    "#         RNN.hm_epochs = 10000\n",
    "        RNN.hm_epochs = 3\n",
    "\n",
    "    def set_Data(RNN, Input, Output, InputNames, LabelsDict, dictionary_inv):\n",
    "\n",
    "        RNN.InputNames = InputNames\n",
    "        RNN.OutputNames = sorted(LabelsDict, key = LabelsDict.get, reverse = False)\n",
    "        RNN.dictionary = LabelsDict\n",
    "        RNN.dictionary_inv = dictionary_inv\n",
    "\n",
    "        RNN.n_samples = Input.shape[2]\n",
    "        RNN.training_size = int(0.8 * RNN.n_samples)\n",
    "        RNN.testing_size = int(0.2 * RNN.n_samples)\n",
    "\n",
    "        Input_N, RNN.meanInput, RNN.stdInput = standardize_data(Input)\n",
    "\n",
    "        RNN.train_x = np.array(Input[:, :, :RNN.training_size]).astype('float32')\n",
    "        RNN.train_x = np.transpose(RNN.train_x, [2, 0, 1])\n",
    "        RNN.train_y = np.array(Output[:RNN.training_size])\n",
    "\n",
    "        RNN.test_x = np.array(Input[:, :, -RNN.testing_size:]).astype('float32')\n",
    "        RNN.test_x = np.transpose(RNN.test_x, [2, 0, 1])\n",
    "        RNN.test_y = np.array(Output[-RNN.testing_size:])\n",
    "\n",
    "        RNN.train_x_N = np.array(Input_N[:, :, :RNN.training_size]).astype('float32')\n",
    "        RNN.train_x_N = np.transpose(RNN.train_x_N, [2, 0, 1])\n",
    "        RNN.test_x_N = np.array(Input_N[:, :, -RNN.testing_size:]).astype('float32')\n",
    "        RNN.test_x_N = np.transpose(RNN.test_x_N, [2, 0, 1])\n",
    "\n",
    "    def train_neural_network(RNN, InputAll, one_hots):\n",
    "\n",
    "        errorHistory =[]\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        sess = tf.Session()\n",
    "\n",
    "        frames = 100\n",
    "#         n_Joint = 162\n",
    "        n_Joint = 58\n",
    "\n",
    "        CellSize = 10\n",
    "#         OutputSize = 20\n",
    "        OutputSize = 3\n",
    "\n",
    "        RNN.x = tf.placeholder('float32',\n",
    "                           [None, frames, n_Joint])  # TensorShape([Dimension(None), Dimension(28), Dimension(28)])\n",
    "        RNN.y = tf.placeholder('float32', [None, OutputSize])\n",
    "\n",
    "\n",
    "        layer = {'weights': tf.Variable(tf.random_normal([CellSize, OutputSize])),\n",
    "                 'biases': tf.Variable(tf.random_normal([OutputSize]))}\n",
    "\n",
    "        x = tf.transpose(RNN.x, [1, 0, 2])  # TensorShape([Dimension(28), Dimension(None), Dimension(28)])\n",
    "        x = tf.reshape(x, [-1, n_Joint])\n",
    "        x = tf.split(x, frames, 0)  # len(x) = 100\n",
    "\n",
    "        lstm_cell = rnn.BasicRNNCell(CellSize)\n",
    "\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        RNN.y_ = tf.matmul(outputs[-1], layer['weights']) + layer['biases']\n",
    "\n",
    "        learningRate = 0.01\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=RNN.y_, labels=RNN.y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(cost)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(RNN.y_, 1), tf.argmax(RNN.y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(RNN.hm_epochs):\n",
    "\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={RNN.x:RNN.train_x_N , RNN.y: RNN.train_y})\n",
    "            errorHistory.append(c)\n",
    "            if epoch > 100:\n",
    "                if epoch % 100 == 0:\n",
    "                    if abs(errorHistory[epoch] - errorHistory[epoch-100]) < 0.01:\n",
    "                        break\n",
    "            print('Epoch', epoch, 'completed out of', RNN.hm_epochs, 'loss:', c)\n",
    "\n",
    "        test_data = {RNN.x: RNN.test_x_N, RNN.y: RNN.test_y}\n",
    "\n",
    "        RNN.accuracy = sess.run(accuracy, feed_dict = test_data)\n",
    "\n",
    "        input_data = {RNN.x: RNN.test_x_N}\n",
    "\n",
    "        RNN.output_rnn = sess.run(RNN.y_, feed_dict=input_data)\n",
    "\n",
    "        RNN.output_probabilities = tf.nn.softmax(RNN.output_rnn)\n",
    "        RNN.output_probabilities = sess.run(RNN.output_probabilities)\n",
    "        RNN.output_probabilities = np.array(RNN.output_probabilities)\n",
    "\n",
    "        RNN.output_OneHot = (RNN.output_probabilities == RNN.output_probabilities.max(axis = 1, keepdims = True)).astype(int)\n",
    "\n",
    "        RNN.output_label = []\n",
    "\n",
    "        for i in range(0, len(RNN.dictionary)):\n",
    "            RNN.output_label_temp = RNN.dictionary_inv.get(np.argmax(RNN.output_OneHot[i]))\n",
    "            RNN.output_label = np.append(RNN.output_label, RNN.output_label_temp)\n",
    "            # RNN.output_label = np.core.defchararray.add(RNN.output_label, RNN.output_label_temp)\n",
    "\n",
    "        print('Accuracy:', RNN.accuracy)\n",
    "        plt.figure()\n",
    "        plt.plot(errorHistory)\n",
    "        plt.title('Loss_value')\n",
    "        plt.savefig('Loss' + RNN.accuracy + '.png')\n",
    "\n",
    "        f = open('loss' + RNN.accuracy + '.csv', 'w')\n",
    "        writer = csv.writer(f, lineterminator='/n')\n",
    "        writer.writerow(errorHistory)\n",
    "        f.close()\n",
    "\n",
    "    def draw_confusion_matrix(RNN):\n",
    "\n",
    "        RNN.test_label = []\n",
    "\n",
    "        for i in range(0, len(RNN.dictionary)):\n",
    "            RNN.test_label_temp = RNN.dictionary_inv.get(np.argmax(RNN.test_y[i]))\n",
    "            RNN.test_label = np.append(RNN.test_label, RNN.test_label_temp)\n",
    "\n",
    "        labels = sorted(list(set(RNN.test_label)))\n",
    "        cmx_data = confusion_matrix(RNN.test_label, RNN.output_label, labels=labels)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            cmx_data_N = np.true_divide(cmx_data, cmx_data.astype(np.float).sum(axis=0))\n",
    "            cmx_data_N = np.nan_to_num(cmx_data_N)\n",
    "\n",
    "        df_cmx = pd.DataFrame(cmx_data.T, index=labels, columns=labels)\n",
    "        df_cmx_N = pd.DataFrame(cmx_data_N.T, index=labels, columns=labels)\n",
    "\n",
    "        np.savetxt('100.txt', cmx_data_N)\n",
    "\n",
    "        # plt.figure()\n",
    "        # sn.heatmap(df_cmx, vmax=1, vmin=0, annot=True, robust=True)\n",
    "        # plt.show()\n",
    "\n",
    "        plt.figure(figsize=(30,30))\n",
    "        sn.heatmap(df_cmx_N, vmax=1, vmin=0, annot=True)\n",
    "        plt.show()\n",
    "        plt.savefig('cmx.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----done.\n",
      "Epoch 0 completed out of 3 loss: 4.799125\n",
      "Epoch 1 completed out of 3 loss: 4.1926184\n",
      "Epoch 2 completed out of 3 loss: 3.5916579\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-4709aa8ad93a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mone_hots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputNames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInputNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelsDict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary_inv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-----done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-204-57aeefbdd0a5>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(RNN, InputAll, one_hots)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_label_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_inv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_OneHot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_label_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# RNN.output_label = np.core.defchararray.add(RNN.output_label, RNN.output_label_temp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "RNN = RNN()\n",
    "RNN.set_Data(Input=InputAll, Output=one_hots, InputNames=InputNames, LabelsDict=dictionary, dictionary_inv=dictionary_inv)\n",
    "print('-----done.')\n",
    "RNN.train_neural_network(InputAll, one_hots)\n",
    "RNN.draw_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnvPy3]",
   "language": "python",
   "name": "conda-env-myEnvPy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
