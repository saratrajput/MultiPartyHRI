{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Collection of functions for scientific and publication-ready visualization\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Visualization library based on matplotlib, provides interface for drawing attractive statistical graphics\n",
    "# import seaborn as sns  # Install seaborn in myEnvPy3 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the split data\n",
    "data = pd.read_csv('./data/kinectData/splitKinectData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To describe the data with count, mean, std, min, etc..\n",
    "\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the top part of data\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the bottom part of the data\n",
    "\n",
    "# data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see a sample of whole data\n",
    "\n",
    "# data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check datatype and other info of each column\n",
    "# By lookin at total number of entries and number of non-null objects we can check \n",
    "# if there are any null values or missing data\n",
    "\n",
    "# data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping 'face_engaged': no = 0; yes = 1\n",
    "# With yes and no: face_looking away, face_engaged\n",
    "\n",
    "yesNoList = ['face_engaged', 'face_lookingaway']\n",
    "\n",
    "for i in range(len(yesNoList)):\n",
    "    data[yesNoList[i]] = data[yesNoList[i]].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)\n",
    "    \n",
    "# Mapping 'face_glasses': unknown & no = 0; yes = 1 ###Commented and created a loop###\n",
    "# data['face_glasses'] = data['face_glasses'].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)\n",
    "\n",
    "# unknown, no yes: face_happy, face_lefteyeclosed, face_mouthmoved, face_mouthopen, face_righteyeclosed\n",
    "unknownYesNoList = ['face_glasses', 'face_happy', 'face_lefteyeclosed', 'face_mouthmoved',\n",
    "                    'face_mouthopen', 'face_righteyeclosed']\n",
    "for i in range(len(unknownYesNoList)):\n",
    "    data[unknownYesNoList[i]] = data[unknownYesNoList[i]].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping face size since it's a column of zeros\n",
    "# data.drop(columns=['face_size'])\n",
    "# data\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping First column of indexes since it is redundant\n",
    "# data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "# data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing individual features\n",
    "# data['pedes_posX'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing the data to a unsupervised learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling features to a range\n",
    "# initialize Min-max scaler\n",
    "minMaxScaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# copy df to be compared later\n",
    "unprocessData = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Data\n",
    "# dataNormalized = preprocessing.normalize(unprocessData, norm='l2')\n",
    "\n",
    "# MinMax Scaling Unprocessed Data\n",
    "dataMinMax = minMaxScaler.fit_transform(unprocessData)\n",
    "\n",
    "# MinMax Scaling Normalized Data\n",
    "# dataMinMax = minMaxScaler.fit_transform(dataNormalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Check if k-value for PCA retains 99% of variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8834624645663419"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Covariance Matrix\n",
    "numTrainEg = dataMinMax.shape\n",
    "covarMatrix = (1/numTrainEg[0])*(np.matmul(dataMinMax.T, dataMinMax))\n",
    "covarMatrix.shape\n",
    "# Get svd of Covariance Matrix\n",
    "u, s, v = np.linalg.svd(covarMatrix, full_matrices=True)\n",
    "# Checking how much variance we have retained\n",
    "sum(s[:2])/sum(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try k-means clustering\n",
    "# import clusters\n",
    "from sklearn import cluster\n",
    "\n",
    "# initialize k-means\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(dataMinMax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = k_means.labels_\n",
    "# centers = k_means.cluster_centers_\n",
    "# help(k_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-191928ec77d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mkmeansReduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# reduced_data = PCA(n_components=2).fit_transform(dataNormalized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "kmeansReduced = cluster.KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "\n",
    "# reduced_data = PCA(n_components=2).fit_transform(dataNormalized)\n",
    "reduced_data = PCA(n_components=2).fit_transform(dataMinMax)\n",
    "\n",
    "kmeansReduced.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeansReduced.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeansReduced.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "# plt.title('K-means clustering on Normalized Data\\n'\n",
    "#           'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# Make a selection\n",
    "# plt.savefig('kMeansMinMaxPCA.eps', format='eps', dpi=100)\n",
    "# plt.savefig('kMeansNormalizedPCA.eps', format='eps', dpi=100)\n",
    "# plt.savefig('kMeansNormMinMaxPCA.eps', format='eps', dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3994836  0.58945631]\n"
     ]
    }
   ],
   "source": [
    "# Only Testing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# kmeansReduced = cluster.KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "\n",
    "# reduced_data = PCA(n_components=2).fit_transform(dataNormalized)\n",
    "reduced_data = PCA(n_components=2)\n",
    "reduced_data.fit_transform(dataMinMax)\n",
    "print(reduced_data.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomoyaData = pd.read_csv('tomoyoLeapCat_100_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>thumbProximal_L_X</th>\n",
       "      <th>thumbProximal_L_Y</th>\n",
       "      <th>thumbProximal_L_Z</th>\n",
       "      <th>thumbDistal_L_X</th>\n",
       "      <th>thumbDistal_L_Y</th>\n",
       "      <th>thumbDistal_L_Z</th>\n",
       "      <th>thumbEF_L_X</th>\n",
       "      <th>thumbEF_L_Y</th>\n",
       "      <th>...</th>\n",
       "      <th>armFrontRadius_R_Z</th>\n",
       "      <th>armFrontUnla_R_X</th>\n",
       "      <th>armFrontUnla_R_Y</th>\n",
       "      <th>armFrontUnla_R_Z</th>\n",
       "      <th>armBackLateral_R_X</th>\n",
       "      <th>armBackLateral_R_Y</th>\n",
       "      <th>armBackLateral_R_Z</th>\n",
       "      <th>armBackMedial_R_X</th>\n",
       "      <th>armBackMedial_R_Y</th>\n",
       "      <th>armBackMedial_R_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.154221</td>\n",
       "      <td>-0.009801</td>\n",
       "      <td>0.351608</td>\n",
       "      <td>-0.154221</td>\n",
       "      <td>-0.009801</td>\n",
       "      <td>0.351608</td>\n",
       "      <td>-0.125315</td>\n",
       "      <td>0.021736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320950</td>\n",
       "      <td>0.071754</td>\n",
       "      <td>-0.031825</td>\n",
       "      <td>0.354184</td>\n",
       "      <td>0.191447</td>\n",
       "      <td>-0.170060</td>\n",
       "      <td>0.207293</td>\n",
       "      <td>0.208151</td>\n",
       "      <td>-0.180164</td>\n",
       "      <td>0.240527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>-0.154094</td>\n",
       "      <td>-0.009933</td>\n",
       "      <td>0.351446</td>\n",
       "      <td>-0.154094</td>\n",
       "      <td>-0.009933</td>\n",
       "      <td>0.351446</td>\n",
       "      <td>-0.125453</td>\n",
       "      <td>0.021754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307211</td>\n",
       "      <td>0.077601</td>\n",
       "      <td>-0.025145</td>\n",
       "      <td>0.341086</td>\n",
       "      <td>0.193232</td>\n",
       "      <td>-0.174480</td>\n",
       "      <td>0.202013</td>\n",
       "      <td>0.209081</td>\n",
       "      <td>-0.183807</td>\n",
       "      <td>0.235887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>-0.154779</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>0.351216</td>\n",
       "      <td>-0.154779</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>0.351216</td>\n",
       "      <td>-0.126826</td>\n",
       "      <td>0.022690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296780</td>\n",
       "      <td>0.208874</td>\n",
       "      <td>-0.014308</td>\n",
       "      <td>0.330060</td>\n",
       "      <td>0.160117</td>\n",
       "      <td>-0.214562</td>\n",
       "      <td>0.218106</td>\n",
       "      <td>0.173198</td>\n",
       "      <td>-0.228937</td>\n",
       "      <td>0.251386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>-0.154391</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>0.350695</td>\n",
       "      <td>-0.154391</td>\n",
       "      <td>-0.009295</td>\n",
       "      <td>0.350695</td>\n",
       "      <td>-0.126037</td>\n",
       "      <td>0.022484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290330</td>\n",
       "      <td>0.178661</td>\n",
       "      <td>-0.004194</td>\n",
       "      <td>0.325022</td>\n",
       "      <td>0.171782</td>\n",
       "      <td>-0.210765</td>\n",
       "      <td>0.214251</td>\n",
       "      <td>0.183721</td>\n",
       "      <td>-0.222570</td>\n",
       "      <td>0.248943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>-0.154457</td>\n",
       "      <td>-0.009486</td>\n",
       "      <td>0.351018</td>\n",
       "      <td>-0.154457</td>\n",
       "      <td>-0.009486</td>\n",
       "      <td>0.351018</td>\n",
       "      <td>-0.126409</td>\n",
       "      <td>0.022589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295726</td>\n",
       "      <td>0.199935</td>\n",
       "      <td>-0.009963</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.164159</td>\n",
       "      <td>-0.211359</td>\n",
       "      <td>0.215317</td>\n",
       "      <td>0.174564</td>\n",
       "      <td>-0.225407</td>\n",
       "      <td>0.249667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Time   thumbProximal_L_X   thumbProximal_L_Y  \\\n",
       "13          13    13           -0.154221           -0.009801   \n",
       "31          31    31           -0.154094           -0.009933   \n",
       "64          64    64           -0.154779           -0.009379   \n",
       "77          77    77           -0.154391           -0.009295   \n",
       "70          70    70           -0.154457           -0.009486   \n",
       "\n",
       "     thumbProximal_L_Z   thumbDistal_L_X   thumbDistal_L_Y   thumbDistal_L_Z  \\\n",
       "13            0.351608         -0.154221         -0.009801          0.351608   \n",
       "31            0.351446         -0.154094         -0.009933          0.351446   \n",
       "64            0.351216         -0.154779         -0.009379          0.351216   \n",
       "77            0.350695         -0.154391         -0.009295          0.350695   \n",
       "70            0.351018         -0.154457         -0.009486          0.351018   \n",
       "\n",
       "     thumbEF_L_X   thumbEF_L_Y         ...           armFrontRadius_R_Z  \\\n",
       "13     -0.125315      0.021736         ...                     0.320950   \n",
       "31     -0.125453      0.021754         ...                     0.307211   \n",
       "64     -0.126826      0.022690         ...                     0.296780   \n",
       "77     -0.126037      0.022484         ...                     0.290330   \n",
       "70     -0.126409      0.022589         ...                     0.295726   \n",
       "\n",
       "     armFrontUnla_R_X   armFrontUnla_R_Y   armFrontUnla_R_Z  \\\n",
       "13           0.071754          -0.031825           0.354184   \n",
       "31           0.077601          -0.025145           0.341086   \n",
       "64           0.208874          -0.014308           0.330060   \n",
       "77           0.178661          -0.004194           0.325022   \n",
       "70           0.199935          -0.009963           0.330075   \n",
       "\n",
       "     armBackLateral_R_X   armBackLateral_R_Y   armBackLateral_R_Z  \\\n",
       "13             0.191447            -0.170060             0.207293   \n",
       "31             0.193232            -0.174480             0.202013   \n",
       "64             0.160117            -0.214562             0.218106   \n",
       "77             0.171782            -0.210765             0.214251   \n",
       "70             0.164159            -0.211359             0.215317   \n",
       "\n",
       "     armBackMedial_R_X   armBackMedial_R_Y   armBackMedial_R_Z  \n",
       "13            0.208151           -0.180164            0.240527  \n",
       "31            0.209081           -0.183807            0.235887  \n",
       "64            0.173198           -0.228937            0.251386  \n",
       "77            0.183721           -0.222570            0.248943  \n",
       "70            0.174564           -0.225407            0.249667  \n",
       "\n",
       "[5 rows x 164 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tomoyaData.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio Data Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop last n rows\n",
    "n = 100\n",
    "data.drop(data.tail(n).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate into three new df\n",
    "approachDf = data.iloc[:300, :]\n",
    "interactDf = data.iloc[300:600, :]\n",
    "leaveDf = data.iloc[600:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "approachDf.to_csv('approachDf' + '.csv')\n",
    "interactDf.to_csv('interactDf' + '.csv')\n",
    "leaveDf.to_csv('leaveDf' + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sp/anaconda3/envs/myEnvPy3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Tomoya part\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from keras.utils import to_categorical\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_2_one_hots(labels=''):\n",
    "    words = []\n",
    "    new_word_id = 0\n",
    "    dictionary = {}\n",
    "    labels = labels[:, np.newaxis]\n",
    "    # sort by alphabetical order\n",
    "    labelsSorted = np.sort(labels, axis=0)\n",
    "\n",
    "    for word in labelsSorted:\n",
    "        if word[0] not in dictionary:\n",
    "            dictionary[word[0]] = new_word_id\n",
    "            new_word_id += 1\n",
    "\n",
    "\n",
    "    for word in labels:\n",
    "        words.append(dictionary[word[0]])\n",
    "\n",
    "    one_hots = to_categorical(words)\n",
    "    dictionary_inv = {dictionary[k]: k for k in dictionary}\n",
    "\n",
    "    return dictionary, one_hots, dictionary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data():\n",
    "\n",
    "    # Get current working directory of process\n",
    "    cwd = os.getcwd()\n",
    "    path = cwd + '/data' + '/deepLearning'\n",
    "    files = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            files.append(file)\n",
    "\n",
    "    files = np.array(files)\n",
    "    labelsOutput = []\n",
    "    InputAll = []\n",
    "\n",
    "    for i in range(0, int(files.shape[0])):\n",
    "        dfInput = pd.read_csv(path + '/' + files[i], sep=',')\n",
    "    #     dfInput.dropna(how='any') # Drop missing values\n",
    "        dfInput.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "        print(files[i] + ' open')\n",
    "        label = str.lower(files[i][:-4]) # Remove '.csv'\n",
    "        InputNames = dfInput.columns\n",
    "\n",
    "        Input = dfInput.as_matrix() # Convert the frame to its Numpy-array representation\n",
    "        nCut = int(Input.shape[0] / 100)  \n",
    "\n",
    "        # Convert from Rows, Columns(2D) to nCut, 100, Columns(3D)\n",
    "        Input = np.array(np.split(Input, nCut, axis=0))\n",
    "        # Transpose to 100, Columns, nCut\n",
    "        Input = Input.transpose([1, 2, 0])\n",
    "\n",
    "        labelsOutput.append([label for j in range(0, nCut)])\n",
    "\n",
    "        if InputAll == []:\n",
    "            InputAll = Input\n",
    "        else:\n",
    "            InputAll = np.concatenate([InputAll, Input], axis=2)\n",
    "\n",
    "    labelsOutput = np.array(list(chain.from_iterable(labelsOutput))) # chain('ABC', 'DEF') --> A B C D E F\n",
    "\n",
    "    p = np.random.permutation(InputAll.shape[2])\n",
    "\n",
    "    InputAll = InputAll[:, :, p]\n",
    "\n",
    "    labelsOutput = labelsOutput[p]\n",
    "\n",
    "    dictionary, one_hots, dictionary_inv = labels_2_one_hots(labelsOutput)\n",
    "    \n",
    "    return InputAll, dictionary, one_hots, InputNames, dictionary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(data):\n",
    "    std = []\n",
    "    mean = []\n",
    "    dataOut = data.copy()\n",
    "\n",
    "    std.append(np.std(data, ddof=1))\n",
    "    mean.append(np.mean(data))\n",
    "    dataOut = (data - mean) / std\n",
    "\n",
    "    dataOut[np.isnan(dataOut)] = 0\n",
    "\n",
    "    return dataOut, mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaveDf.csv open\n",
      "approachDf.csv open\n",
      "interactDf.csv open\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/envs/myEnvPy3/lib/python3.6/site-packages/ipykernel/__main__.py:33: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "InputAll, dictionary, one_hots, InputNames, dictionary_inv = get_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(RNN):\n",
    "\n",
    "#         RNN.hm_epochs = 10000\n",
    "        RNN.hm_epochs = 3\n",
    "\n",
    "    def set_Data(RNN, Input, Output, InputNames, LabelsDict, dictionary_inv):\n",
    "\n",
    "        RNN.InputNames = InputNames\n",
    "        RNN.OutputNames = sorted(LabelsDict, key = LabelsDict.get, reverse = False)\n",
    "        RNN.dictionary = LabelsDict\n",
    "        RNN.dictionary_inv = dictionary_inv\n",
    "\n",
    "        RNN.n_samples = Input.shape[2]\n",
    "        RNN.training_size = int(0.8 * RNN.n_samples)\n",
    "        RNN.testing_size = int(0.2 * RNN.n_samples)\n",
    "\n",
    "        Input_N, RNN.meanInput, RNN.stdInput = standardize_data(Input)\n",
    "\n",
    "        RNN.train_x = np.array(Input[:, :, :RNN.training_size]).astype('float32')\n",
    "        RNN.train_x = np.transpose(RNN.train_x, [2, 0, 1])\n",
    "        RNN.train_y = np.array(Output[:RNN.training_size])\n",
    "\n",
    "        RNN.test_x = np.array(Input[:, :, -RNN.testing_size:]).astype('float32')\n",
    "        RNN.test_x = np.transpose(RNN.test_x, [2, 0, 1])\n",
    "        RNN.test_y = np.array(Output[-RNN.testing_size:])\n",
    "\n",
    "        RNN.train_x_N = np.array(Input_N[:, :, :RNN.training_size]).astype('float32')\n",
    "        RNN.train_x_N = np.transpose(RNN.train_x_N, [2, 0, 1])\n",
    "        RNN.test_x_N = np.array(Input_N[:, :, -RNN.testing_size:]).astype('float32')\n",
    "        RNN.test_x_N = np.transpose(RNN.test_x_N, [2, 0, 1])\n",
    "\n",
    "    def train_neural_network(RNN, InputAll, one_hots):\n",
    "\n",
    "        errorHistory =[]\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        sess = tf.Session()\n",
    "\n",
    "        frames = 100\n",
    "#         n_Joint = 162\n",
    "        n_Joint = 58\n",
    "\n",
    "        CellSize = 10\n",
    "#         OutputSize = 20\n",
    "        OutputSize = 3\n",
    "\n",
    "        RNN.x = tf.placeholder('float32',\n",
    "                           [None, frames, n_Joint])  # TensorShape([Dimension(None), Dimension(28), Dimension(28)])\n",
    "        RNN.y = tf.placeholder('float32', [None, OutputSize])\n",
    "\n",
    "\n",
    "        layer = {'weights': tf.Variable(tf.random_normal([CellSize, OutputSize])),\n",
    "                 'biases': tf.Variable(tf.random_normal([OutputSize]))}\n",
    "\n",
    "        x = tf.transpose(RNN.x, [1, 0, 2])  # TensorShape([Dimension(28), Dimension(None), Dimension(28)])\n",
    "        x = tf.reshape(x, [-1, n_Joint])\n",
    "        x = tf.split(x, frames, 0)  # len(x) = 100\n",
    "\n",
    "        lstm_cell = rnn.BasicRNNCell(CellSize)\n",
    "\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        RNN.y_ = tf.matmul(outputs[-1], layer['weights']) + layer['biases']\n",
    "\n",
    "        learningRate = 0.01\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=RNN.y_, labels=RNN.y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(cost)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(RNN.y_, 1), tf.argmax(RNN.y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(RNN.hm_epochs):\n",
    "\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={RNN.x:RNN.train_x_N , RNN.y: RNN.train_y})\n",
    "            errorHistory.append(c)\n",
    "            if epoch > 100:\n",
    "                if epoch % 100 == 0:\n",
    "                    if abs(errorHistory[epoch] - errorHistory[epoch-100]) < 0.01:\n",
    "                        break\n",
    "            print('Epoch', epoch, 'completed out of', RNN.hm_epochs, 'loss:', c)\n",
    "\n",
    "        test_data = {RNN.x: RNN.test_x_N, RNN.y: RNN.test_y}\n",
    "\n",
    "        RNN.accuracy = sess.run(accuracy, feed_dict = test_data)\n",
    "\n",
    "        input_data = {RNN.x: RNN.test_x_N}\n",
    "\n",
    "        RNN.output_rnn = sess.run(RNN.y_, feed_dict=input_data)\n",
    "\n",
    "        RNN.output_probabilities = tf.nn.softmax(RNN.output_rnn)\n",
    "        RNN.output_probabilities = sess.run(RNN.output_probabilities)\n",
    "        RNN.output_probabilities = np.array(RNN.output_probabilities)\n",
    "\n",
    "        RNN.output_OneHot = (RNN.output_probabilities == RNN.output_probabilities.max(axis = 1, keepdims = True)).astype(int)\n",
    "\n",
    "        RNN.output_label = []\n",
    "\n",
    "        for i in range(0, len(RNN.dictionary)):\n",
    "            RNN.output_label_temp = RNN.dictionary_inv.get(np.argmax(RNN.output_OneHot[i]))\n",
    "            RNN.output_label = np.append(RNN.output_label, RNN.output_label_temp)\n",
    "            # RNN.output_label = np.core.defchararray.add(RNN.output_label, RNN.output_label_temp)\n",
    "\n",
    "        print('Accuracy:', RNN.accuracy)\n",
    "        plt.figure()\n",
    "        plt.plot(errorHistory)\n",
    "        plt.title('Loss_value')\n",
    "        plt.savefig('Loss' + RNN.accuracy + '.png')\n",
    "\n",
    "        f = open('loss' + RNN.accuracy + '.csv', 'w')\n",
    "        writer = csv.writer(f, lineterminator='/n')\n",
    "        writer.writerow(errorHistory)\n",
    "        f.close()\n",
    "\n",
    "    def draw_confusion_matrix(RNN):\n",
    "\n",
    "        RNN.test_label = []\n",
    "\n",
    "        for i in range(0, len(RNN.dictionary)):\n",
    "            RNN.test_label_temp = RNN.dictionary_inv.get(np.argmax(RNN.test_y[i]))\n",
    "            RNN.test_label = np.append(RNN.test_label, RNN.test_label_temp)\n",
    "\n",
    "        labels = sorted(list(set(RNN.test_label)))\n",
    "        cmx_data = confusion_matrix(RNN.test_label, RNN.output_label, labels=labels)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            cmx_data_N = np.true_divide(cmx_data, cmx_data.astype(np.float).sum(axis=0))\n",
    "            cmx_data_N = np.nan_to_num(cmx_data_N)\n",
    "\n",
    "        df_cmx = pd.DataFrame(cmx_data.T, index=labels, columns=labels)\n",
    "        df_cmx_N = pd.DataFrame(cmx_data_N.T, index=labels, columns=labels)\n",
    "\n",
    "        np.savetxt('100.txt', cmx_data_N)\n",
    "\n",
    "        # plt.figure()\n",
    "        # sn.heatmap(df_cmx, vmax=1, vmin=0, annot=True, robust=True)\n",
    "        # plt.show()\n",
    "\n",
    "        plt.figure(figsize=(30,30))\n",
    "        sn.heatmap(df_cmx_N, vmax=1, vmin=0, annot=True)\n",
    "        plt.show()\n",
    "        plt.savefig('cmx.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----done.\n",
      "WARNING:tensorflow:From <ipython-input-10-57aeefbdd0a5>:68: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch 0 completed out of 3 loss: 2.6970484\n",
      "Epoch 1 completed out of 3 loss: 1.9913989\n",
      "Epoch 2 completed out of 3 loss: 1.3917367\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4709aa8ad93a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mone_hots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputNames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInputNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelsDict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary_inv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-----done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-57aeefbdd0a5>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(RNN, InputAll, one_hots)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_label_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary_inv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_OneHot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_label_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# RNN.output_label = np.core.defchararray.add(RNN.output_label, RNN.output_label_temp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "RNN = RNN()\n",
    "RNN.set_Data(Input=InputAll, Output=one_hots, InputNames=InputNames, LabelsDict=dictionary, dictionary_inv=dictionary_inv)\n",
    "print('-----done.')\n",
    "RNN.train_neural_network(InputAll, one_hots)\n",
    "RNN.draw_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 1D or 2D array, got 3D array instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ed695c88cb29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/sp/multiPartyHRI/data_har.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/sp/multiPartyHRI\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/myEnvPy3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1325\u001b[0;31m                 \"Expected 1D or 2D array, got %dD array instead\" % X.ndim)\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;31m# Common case -- 1d array of numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1D or 2D array, got 3D array instead"
     ]
    }
   ],
   "source": [
    "data = np.load(\"/home/sp/multiPartyHRI/data_har.npz\")\n",
    "for key, value in data.items():\n",
    "    np.savetxt(\"/home/sp/multiPartyHRI\" + key + \".csv\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/kinectData/splitKinectData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(dataFrame):\n",
    "    # Mapping 'face_engaged': no = 0; yes = 1\n",
    "    # With yes and no: face_looking away, face_engaged\n",
    "\n",
    "    yesNoList = ['face_engaged', 'face_lookingaway']\n",
    "\n",
    "    for i in range(len(yesNoList)):\n",
    "        data[yesNoList[i]] = data[yesNoList[i]].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)\n",
    "\n",
    "    # Mapping 'face_glasses': unknown & no = 0; yes = 1 ###Commented and created a loop###\n",
    "    # data['face_glasses'] = data['face_glasses'].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)\n",
    "\n",
    "    # unknown, no yes: face_happy, face_lefteyeclosed, face_mouthmoved, face_mouthopen, face_righteyeclosed\n",
    "    unknownYesNoList = ['face_glasses', 'face_happy', 'face_lefteyeclosed', 'face_mouthmoved',\n",
    "                        'face_mouthopen', 'face_righteyeclosed']\n",
    "    for i in range(len(unknownYesNoList)):\n",
    "        data[unknownYesNoList[i]] = data[unknownYesNoList[i]].map( {'no': 0, 'yes': 1, 'unknown': 0} ).astype(int)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>face_engaged</th>\n",
       "      <th>face_glasses</th>\n",
       "      <th>face_happy</th>\n",
       "      <th>face_lefteyeclosed</th>\n",
       "      <th>face_lookingaway</th>\n",
       "      <th>face_mouthmoved</th>\n",
       "      <th>face_mouthopen</th>\n",
       "      <th>face_pitch</th>\n",
       "      <th>face_righteyeclosed</th>\n",
       "      <th>...</th>\n",
       "      <th>WristLeftX</th>\n",
       "      <th>WristLeftY</th>\n",
       "      <th>WristLeftZ</th>\n",
       "      <th>WristRightX</th>\n",
       "      <th>WristRightY</th>\n",
       "      <th>WristRightZ</th>\n",
       "      <th>pedes_posX</th>\n",
       "      <th>pedes_posY</th>\n",
       "      <th>pedes_posZ</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032113</td>\n",
       "      <td>-0.029377</td>\n",
       "      <td>1.423275</td>\n",
       "      <td>0.498121</td>\n",
       "      <td>0.027920</td>\n",
       "      <td>1.141602</td>\n",
       "      <td>0.261607</td>\n",
       "      <td>0.313268</td>\n",
       "      <td>1.398278</td>\n",
       "      <td>Approaching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045695</td>\n",
       "      <td>-0.022878</td>\n",
       "      <td>1.430951</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.031860</td>\n",
       "      <td>1.141188</td>\n",
       "      <td>0.264427</td>\n",
       "      <td>0.310272</td>\n",
       "      <td>1.405187</td>\n",
       "      <td>Approaching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045695</td>\n",
       "      <td>-0.022878</td>\n",
       "      <td>1.430951</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.031860</td>\n",
       "      <td>1.141188</td>\n",
       "      <td>0.264427</td>\n",
       "      <td>0.310272</td>\n",
       "      <td>1.405187</td>\n",
       "      <td>Approaching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063713</td>\n",
       "      <td>-0.013085</td>\n",
       "      <td>1.436108</td>\n",
       "      <td>0.483712</td>\n",
       "      <td>0.038925</td>\n",
       "      <td>1.147216</td>\n",
       "      <td>0.266878</td>\n",
       "      <td>0.308362</td>\n",
       "      <td>1.420895</td>\n",
       "      <td>Approaching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063713</td>\n",
       "      <td>-0.013085</td>\n",
       "      <td>1.436108</td>\n",
       "      <td>0.483712</td>\n",
       "      <td>0.038925</td>\n",
       "      <td>1.147216</td>\n",
       "      <td>0.266878</td>\n",
       "      <td>0.308362</td>\n",
       "      <td>1.420895</td>\n",
       "      <td>Approaching</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  face_engaged  face_glasses  face_happy  face_lefteyeclosed  \\\n",
       "0           0             0             0           0                   0   \n",
       "1           1             0             0           0                   0   \n",
       "2           2             0             0           0                   0   \n",
       "3           3             0             1           0                   1   \n",
       "4           4             0             1           0                   1   \n",
       "\n",
       "   face_lookingaway  face_mouthmoved  face_mouthopen  face_pitch  \\\n",
       "0                 1                0               0          30   \n",
       "1                 1                0               0          30   \n",
       "2                 1                0               0          30   \n",
       "3                 1                1               0          25   \n",
       "4                 1                1               0          25   \n",
       "\n",
       "   face_righteyeclosed     ...       WristLeftX  WristLeftY  WristLeftZ  \\\n",
       "0                    0     ...        -0.032113   -0.029377    1.423275   \n",
       "1                    0     ...        -0.045695   -0.022878    1.430951   \n",
       "2                    0     ...        -0.045695   -0.022878    1.430951   \n",
       "3                    1     ...        -0.063713   -0.013085    1.436108   \n",
       "4                    1     ...        -0.063713   -0.013085    1.436108   \n",
       "\n",
       "   WristRightX  WristRightY  WristRightZ  pedes_posX  pedes_posY  pedes_posZ  \\\n",
       "0     0.498121     0.027920     1.141602    0.261607    0.313268    1.398278   \n",
       "1     0.489796     0.031860     1.141188    0.264427    0.310272    1.405187   \n",
       "2     0.489796     0.031860     1.141188    0.264427    0.310272    1.405187   \n",
       "3     0.483712     0.038925     1.147216    0.266878    0.308362    1.420895   \n",
       "4     0.483712     0.038925     1.147216    0.266878    0.308362    1.420895   \n",
       "\n",
       "      Activity  \n",
       "0  Approaching  \n",
       "1  Approaching  \n",
       "2  Approaching  \n",
       "3  Approaching  \n",
       "4  Approaching  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = splitData(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureNames = list(data.columns)\n",
    "featureNames.remove(\"Activity\")\n",
    "len(featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, how='any', inplace= True)\n",
    "for i in range(1,len(featureNames)):\n",
    "    data[featureNames[i]] = feature_normalize(data[featureNames[i]]) \n",
    "# seg1 = np.dstack([x[i] for i in range(numFeatures)])\n",
    "# data['WristLeftX'] = feature_normalize(data['WristLeftX'])\n",
    "# data['WristLeftY'] = feature_normalize(data['WristLeftY'])\n",
    "# data['WristLeftZ'] = feature_normalize(data['WristLeftZ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activity(activity,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (10, 8), sharex = True)\n",
    "    plot_axis(ax0, data['Unnamed: 0'], data['WristLeftX'], 'WristLeftX')\n",
    "    plot_axis(ax1, data['Unnamed: 0'], data['WristLeftY'], 'WristLeftY')\n",
    "    plot_axis(ax2, data['Unnamed: 0'], data['WristLeftZ'], 'WristLeftZ')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(activity)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.savefig('foo.jpg')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for activity in np.unique(data[\"Activity\"]):\n",
    "#     subset = data[data[\"Activity\"] == activity][:100]\n",
    "#     plot_activity(activity,subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures = 58\n",
    "\n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "        \n",
    "def segment_signal(data,window_size = 90):\n",
    "    segments = np.empty((0,window_size, numFeatures))\n",
    "    labels = np.empty((0))\n",
    "    x=[]\n",
    "    for (start, end) in windows(data[\"Unnamed: 0\"], window_size):\n",
    "        for i in range(len(featureNames)):\n",
    "            x.append(data[featureNames[i]][start:end])\n",
    "#         x = data[\"WristLeftX\"][start:end]\n",
    "#         y = data[\"WristLeftY\"][start:end]\n",
    "#         z = data[\"WristLeftZ\"][start:end]\n",
    "        if(len(data[\"Unnamed: 0\"][start:end]) == window_size):\n",
    "#             segments = np.vstack(x)\n",
    "#             for i in range(len(featureNames)):\n",
    "#                 seg1 = np.dstack([seg1, x[i]])\n",
    "            seg1 = np.dstack([x[i] for i in range(numFeatures)])\n",
    "            segments = np.vstack([segments, seg1])\n",
    "            \n",
    "#             segments = np.vstack([segments, np.dstack([ x[0], x[1], x[2], x[3], x[4] ])])\n",
    "            labels = np.append(labels,stats.mode(data[\"Activity\"][start:end])[0][0])\n",
    "    return x, segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sp/anaconda3/envs/myEnvPy3/lib/python3.6/site-packages/scipy/stats/stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21, 90, 58)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, segments, labels = segment_signal(data)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,90,58)\n",
    "segments.shape\n",
    "# len(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 1, 90, 58)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 3\n",
    "num_channels = 58\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 100\n",
    "\n",
    "total_batchs = train_x.shape[0] // batch_size\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\t\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\t\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  16.281494140625  Training Accuracy:  0.13333334\n",
      "Epoch:  1  Training Loss:  27.94637680053711  Training Accuracy:  0.13333334\n",
      "Epoch:  2  Training Loss:  35.75899124145508  Training Accuracy:  0.13333334\n",
      "Epoch:  3  Training Loss:  40.96529293060303  Training Accuracy:  0.73333335\n",
      "Epoch:  4  Training Loss:  44.87679386138916  Training Accuracy:  0.73333335\n",
      "Epoch:  5  Training Loss:  48.244709968566895  Training Accuracy:  0.73333335\n",
      "Epoch:  6  Training Loss:  51.3695650100708  Training Accuracy:  0.73333335\n",
      "Epoch:  7  Training Loss:  54.36828136444092  Training Accuracy:  0.73333335\n",
      "Epoch:  8  Training Loss:  57.291282176971436  Training Accuracy:  0.73333335\n",
      "Epoch:  9  Training Loss:  60.16308903694153  Training Accuracy:  0.73333335\n",
      "Epoch:  10  Training Loss:  62.99706721305847  Training Accuracy:  0.73333335\n",
      "Epoch:  11  Training Loss:  65.80129742622375  Training Accuracy:  0.73333335\n",
      "Epoch:  12  Training Loss:  68.581139087677  Training Accuracy:  0.73333335\n",
      "Epoch:  13  Training Loss:  71.34041213989258  Training Accuracy:  0.73333335\n",
      "Epoch:  14  Training Loss:  74.08200359344482  Training Accuracy:  0.73333335\n",
      "Epoch:  15  Training Loss:  76.80818486213684  Training Accuracy:  0.73333335\n",
      "Epoch:  16  Training Loss:  79.52079606056213  Training Accuracy:  0.73333335\n",
      "Epoch:  17  Training Loss:  82.22135996818542  Training Accuracy:  0.73333335\n",
      "Epoch:  18  Training Loss:  84.9111533164978  Training Accuracy:  0.73333335\n",
      "Epoch:  19  Training Loss:  87.59126377105713  Training Accuracy:  0.73333335\n",
      "Epoch:  20  Training Loss:  90.26262283325195  Training Accuracy:  0.73333335\n",
      "Epoch:  21  Training Loss:  92.926034450531  Training Accuracy:  0.73333335\n",
      "Epoch:  22  Training Loss:  95.58219861984253  Training Accuracy:  0.73333335\n",
      "Epoch:  23  Training Loss:  98.23172879219055  Training Accuracy:  0.73333335\n",
      "Epoch:  24  Training Loss:  100.8751630783081  Training Accuracy:  0.73333335\n",
      "Epoch:  25  Training Loss:  103.51297950744629  Training Accuracy:  0.73333335\n",
      "Epoch:  26  Training Loss:  106.14560079574585  Training Accuracy:  0.73333335\n",
      "Epoch:  27  Training Loss:  108.7734043598175  Training Accuracy:  0.73333335\n",
      "Epoch:  28  Training Loss:  111.39672946929932  Training Accuracy:  0.73333335\n",
      "Epoch:  29  Training Loss:  114.01588010787964  Training Accuracy:  0.73333335\n",
      "Epoch:  30  Training Loss:  116.6311309337616  Training Accuracy:  0.73333335\n",
      "Epoch:  31  Training Loss:  119.24273037910461  Training Accuracy:  0.73333335\n",
      "Epoch:  32  Training Loss:  121.85090351104736  Training Accuracy:  0.73333335\n",
      "Epoch:  33  Training Loss:  124.45585656166077  Training Accuracy:  0.73333335\n",
      "Epoch:  34  Training Loss:  127.0577781200409  Training Accuracy:  0.73333335\n",
      "Epoch:  35  Training Loss:  129.65683913230896  Training Accuracy:  0.73333335\n",
      "Epoch:  36  Training Loss:  132.25319719314575  Training Accuracy:  0.73333335\n",
      "Epoch:  37  Training Loss:  134.8469979763031  Training Accuracy:  0.73333335\n",
      "Epoch:  38  Training Loss:  137.43837475776672  Training Accuracy:  0.73333335\n",
      "Epoch:  39  Training Loss:  140.02745127677917  Training Accuracy:  0.73333335\n",
      "Epoch:  40  Training Loss:  142.61434316635132  Training Accuracy:  0.73333335\n",
      "Epoch:  41  Training Loss:  145.19915533065796  Training Accuracy:  0.73333335\n",
      "Epoch:  42  Training Loss:  147.781986951828  Training Accuracy:  0.73333335\n",
      "Epoch:  43  Training Loss:  150.36293029785156  Training Accuracy:  0.73333335\n",
      "Epoch:  44  Training Loss:  152.94207048416138  Training Accuracy:  0.73333335\n",
      "Epoch:  45  Training Loss:  155.51948738098145  Training Accuracy:  0.73333335\n",
      "Epoch:  46  Training Loss:  158.0952558517456  Training Accuracy:  0.73333335\n",
      "Epoch:  47  Training Loss:  160.6694450378418  Training Accuracy:  0.73333335\n",
      "Epoch:  48  Training Loss:  163.2421214580536  Training Accuracy:  0.73333335\n",
      "Epoch:  49  Training Loss:  165.81334805488586  Training Accuracy:  0.73333335\n",
      "Epoch:  50  Training Loss:  168.3831810951233  Training Accuracy:  0.73333335\n",
      "Epoch:  51  Training Loss:  170.95167589187622  Training Accuracy:  0.73333335\n",
      "Epoch:  52  Training Loss:  173.51888394355774  Training Accuracy:  0.73333335\n",
      "Epoch:  53  Training Loss:  176.08485388755798  Training Accuracy:  0.73333335\n",
      "Epoch:  54  Training Loss:  178.64963126182556  Training Accuracy:  0.73333335\n",
      "Epoch:  55  Training Loss:  181.21325969696045  Training Accuracy:  0.73333335\n",
      "Epoch:  56  Training Loss:  183.775780916214  Training Accuracy:  0.73333335\n",
      "Epoch:  57  Training Loss:  186.33723282814026  Training Accuracy:  0.73333335\n",
      "Epoch:  58  Training Loss:  188.89765310287476  Training Accuracy:  0.73333335\n",
      "Epoch:  59  Training Loss:  191.45707654953003  Training Accuracy:  0.73333335\n",
      "Epoch:  60  Training Loss:  194.01553606987  Training Accuracy:  0.73333335\n",
      "Epoch:  61  Training Loss:  196.57306289672852  Training Accuracy:  0.73333335\n",
      "Epoch:  62  Training Loss:  199.1296887397766  Training Accuracy:  0.73333335\n",
      "Epoch:  63  Training Loss:  201.68544125556946  Training Accuracy:  0.73333335\n",
      "Epoch:  64  Training Loss:  204.2403473854065  Training Accuracy:  0.73333335\n",
      "Epoch:  65  Training Loss:  206.79443359375  Training Accuracy:  0.73333335\n",
      "Epoch:  66  Training Loss:  209.34772539138794  Training Accuracy:  0.73333335\n",
      "Epoch:  67  Training Loss:  211.90024590492249  Training Accuracy:  0.73333335\n",
      "Epoch:  68  Training Loss:  214.45201778411865  Training Accuracy:  0.73333335\n",
      "Epoch:  69  Training Loss:  217.00306391716003  Training Accuracy:  0.73333335\n",
      "Epoch:  70  Training Loss:  219.55340456962585  Training Accuracy:  0.73333335\n",
      "Epoch:  71  Training Loss:  222.1030592918396  Training Accuracy:  0.73333335\n",
      "Epoch:  72  Training Loss:  224.65204668045044  Training Accuracy:  0.73333335\n",
      "Epoch:  73  Training Loss:  227.20038628578186  Training Accuracy:  0.73333335\n",
      "Epoch:  74  Training Loss:  229.74809527397156  Training Accuracy:  0.73333335\n",
      "Epoch:  75  Training Loss:  232.2951889038086  Training Accuracy:  0.73333335\n",
      "Epoch:  76  Training Loss:  234.84168577194214  Training Accuracy:  0.73333335\n",
      "Epoch:  77  Training Loss:  237.38760042190552  Training Accuracy:  0.73333335\n",
      "Epoch:  78  Training Loss:  239.93294763565063  Training Accuracy:  0.73333335\n",
      "Epoch:  79  Training Loss:  242.4777431488037  Training Accuracy:  0.73333335\n",
      "Epoch:  80  Training Loss:  245.02199864387512  Training Accuracy:  0.73333335\n",
      "Epoch:  81  Training Loss:  247.56572890281677  Training Accuracy:  0.73333335\n",
      "Epoch:  82  Training Loss:  250.1089470386505  Training Accuracy:  0.73333335\n",
      "Epoch:  83  Training Loss:  252.65166449546814  Training Accuracy:  0.73333335\n",
      "Epoch:  84  Training Loss:  255.19389414787292  Training Accuracy:  0.73333335\n",
      "Epoch:  85  Training Loss:  257.7356469631195  Training Accuracy:  0.73333335\n",
      "Epoch:  86  Training Loss:  260.2769339084625  Training Accuracy:  0.73333335\n",
      "Epoch:  87  Training Loss:  262.8177661895752  Training Accuracy:  0.73333335\n",
      "Epoch:  88  Training Loss:  265.3581540584564  Training Accuracy:  0.73333335\n",
      "Epoch:  89  Training Loss:  267.89810729026794  Training Accuracy:  0.73333335\n",
      "Epoch:  90  Training Loss:  270.4376356601715  Training Accuracy:  0.73333335\n",
      "Epoch:  91  Training Loss:  272.9767484664917  Training Accuracy:  0.73333335\n",
      "Epoch:  92  Training Loss:  275.5154550075531  Training Accuracy:  0.73333335\n",
      "Epoch:  93  Training Loss:  278.053763628006  Training Accuracy:  0.73333335\n",
      "Epoch:  94  Training Loss:  280.5916841030121  Training Accuracy:  0.73333335\n",
      "Epoch:  95  Training Loss:  283.12922382354736  Training Accuracy:  0.73333335\n",
      "Epoch:  96  Training Loss:  285.66639018058777  Training Accuracy:  0.73333335\n",
      "Epoch:  97  Training Loss:  288.20319151878357  Training Accuracy:  0.73333335\n",
      "Epoch:  98  Training Loss:  290.7396352291107  Training Accuracy:  0.73333335\n",
      "Epoch:  99  Training Loss:  293.27572870254517  Training Accuracy:  0.73333335\n",
      "Testing Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history = np.empty(shape=[1],dtype=float)\n",
    "        for b in range(total_batchs):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print (\"Epoch: \",epoch,\" Training Loss: \",np.mean(cost_history),\" Training Accuracy: \",\n",
    "              session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    \n",
    "    print (\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() arg 1 must be a string, bytes or code object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-12758d466f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Gyro'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Acce'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Time'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: eval() arg 1 must be a string, bytes or code object"
     ]
    }
   ],
   "source": [
    "line = {'Gyro': [1,2,3], 'Acce': [2,3,4], 'Time': 123}\n",
    "print(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnvPy3]",
   "language": "python",
   "name": "conda-env-myEnvPy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
