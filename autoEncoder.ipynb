{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE_ts_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class variable_scope in module tensorflow.python.ops.variable_scope:\n",
      "\n",
      "class variable_scope(builtins.object)\n",
      " |  A context manager for defining ops that creates variables (layers).\n",
      " |  \n",
      " |  This context manager validates that the (optional) `values` are from the same\n",
      " |  graph, ensures that graph is the default graph, and pushes a name scope and a\n",
      " |  variable scope.\n",
      " |  \n",
      " |  If `name_or_scope` is not None, it is used as is. If `name_or_scope` is None,\n",
      " |  then `default_name` is used.  In that case, if the same name has been\n",
      " |  previously used in the same scope, it will be made unique by appending `_N`\n",
      " |  to it.\n",
      " |  \n",
      " |  Variable scope allows you to create new variables and to share already created\n",
      " |  ones while providing checks to not create or share by accident. For details,\n",
      " |  see the @{$variables$Variable Scope How To}, here we present only a few basic\n",
      " |  examples.\n",
      " |  \n",
      " |  Simple example of how to create a new variable:\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\"):\n",
      " |      with tf.variable_scope(\"bar\"):\n",
      " |          v = tf.get_variable(\"v\", [1])\n",
      " |          assert v.name == \"foo/bar/v:0\"\n",
      " |  ```\n",
      " |  \n",
      " |  Basic example of sharing a variable AUTO_REUSE:\n",
      " |  \n",
      " |  ```python\n",
      " |  def foo():\n",
      " |    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |    return v\n",
      " |  \n",
      " |  v1 = foo()  # Creates v.\n",
      " |  v2 = foo()  # Gets the same, existing v.\n",
      " |  assert v1 == v2\n",
      " |  ```\n",
      " |  \n",
      " |  Basic example of sharing a variable with reuse=True:\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\"):\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |  with tf.variable_scope(\"foo\", reuse=True):\n",
      " |      v1 = tf.get_variable(\"v\", [1])\n",
      " |  assert v1 == v\n",
      " |  ```\n",
      " |  \n",
      " |  Sharing a variable by capturing a scope and setting reuse:\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\") as scope:\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |      scope.reuse_variables()\n",
      " |      v1 = tf.get_variable(\"v\", [1])\n",
      " |  assert v1 == v\n",
      " |  ```\n",
      " |  \n",
      " |  To prevent accidental sharing of variables, we raise an exception when getting\n",
      " |  an existing variable in a non-reusing scope.\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\"):\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |      v1 = tf.get_variable(\"v\", [1])\n",
      " |      #  Raises ValueError(\"... v already exists ...\").\n",
      " |  ```\n",
      " |  \n",
      " |  Similarly, we raise an exception when trying to get a variable that does not\n",
      " |  exist in reuse mode.\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\", reuse=True):\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |      #  Raises ValueError(\"... v does not exists ...\").\n",
      " |  ```\n",
      " |  \n",
      " |  Note that the `reuse` flag is inherited: if we open a reusing scope, then all\n",
      " |  its sub-scopes become reusing as well.\n",
      " |  \n",
      " |  A note about name scoping: Setting `reuse` does not impact the naming of other\n",
      " |  ops such as mult. See related discussion on\n",
      " |  [github#6189](https://github.com/tensorflow/tensorflow/issues/6189)\n",
      " |  \n",
      " |  Note that up to and including version 1.0, it was allowed (though explicitly\n",
      " |  discouraged) to pass False to the reuse argument, yielding undocumented\n",
      " |  behaviour slightly different from None. Starting at 1.1.0 passing None and\n",
      " |  False as reuse has exactly the same effect.\n",
      " |  \n",
      " |  A note about using variable scopes in multi-threaded environment: Variable\n",
      " |  scopes are thread local, so one thread will not see another thread's current\n",
      " |  scope. Also, when using `default_name`, unique scopes names are also generated\n",
      " |  only on a per thread basis. If the same name was used within a different\n",
      " |  thread, that doesn't prevent a new thread from creating the same scope.\n",
      " |  However, the underlying variable store is shared across threads (within the\n",
      " |  same graph). As such, if another thread tries to create a new variable with\n",
      " |  the same name as a variable created by a previous thread, it will fail unless\n",
      " |  reuse is True.\n",
      " |  \n",
      " |  Further, each thread starts with an empty variable scope. So if you wish to\n",
      " |  preserve name prefixes from a scope from the main thread, you should capture\n",
      " |  the main thread's scope and re-enter it in each thread. For e.g.\n",
      " |  \n",
      " |  ```\n",
      " |  main_thread_scope = variable_scope.get_variable_scope()\n",
      " |  \n",
      " |  # Thread's target function:\n",
      " |  def thread_target_fn(captured_scope):\n",
      " |    with variable_scope.variable_scope(captured_scope):\n",
      " |      # .... regular code for this thread\n",
      " |  \n",
      " |  \n",
      " |  thread = threading.Thread(target=thread_target_fn, args=(main_thread_scope,))\n",
      " |  ```\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, type_arg, value_arg, traceback_arg)\n",
      " |  \n",
      " |  __init__(self, name_or_scope, default_name=None, values=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None, use_resource=None, constraint=None, auxiliary_name_scope=True)\n",
      " |      Initialize the context manager.\n",
      " |      \n",
      " |      Args:\n",
      " |        name_or_scope: `string` or `VariableScope`: the scope to open.\n",
      " |        default_name: The default name to use if the `name_or_scope` argument is\n",
      " |          `None`, this name will be uniquified. If name_or_scope is provided it\n",
      " |          won't be used and therefore it is not required and can be None.\n",
      " |        values: The list of `Tensor` arguments that are passed to the op function.\n",
      " |        initializer: default initializer for variables within this scope.\n",
      " |        regularizer: default regularizer for variables within this scope.\n",
      " |        caching_device: default caching device for variables within this scope.\n",
      " |        partitioner: default partitioner for variables within this scope.\n",
      " |        custom_getter: default custom getter for variables within this scope.\n",
      " |        reuse: `True`, None, or tf.AUTO_REUSE; if `True`, we go into reuse mode\n",
      " |          for this scope as well as all sub-scopes; if tf.AUTO_REUSE, we create\n",
      " |          variables if they do not exist, and return them otherwise; if None, we\n",
      " |          inherit the parent scope's reuse flag. When eager execution is enabled,\n",
      " |          this argument is always forced to be tf.AUTO_REUSE.\n",
      " |        dtype: type of variables created in this scope (defaults to the type\n",
      " |          in the passed scope, or inherited from parent scope).\n",
      " |        use_resource: If False, all variables will be regular Variables. If True,\n",
      " |          experimental ResourceVariables with well-defined semantics will be used\n",
      " |          instead. Defaults to False (will later change to True). When eager\n",
      " |          execution is enabled this argument is always forced to be True.\n",
      " |        constraint: An optional projection function to be applied to the variable\n",
      " |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
      " |          constraints or value constraints for layer weights). The function must\n",
      " |          take as input the unprojected Tensor representing the value of the\n",
      " |          variable and return the Tensor for the projected value\n",
      " |          (which must have the same shape). Constraints are not safe to\n",
      " |          use when doing asynchronous distributed training.\n",
      " |        auxiliary_name_scope: If `True`, we create an auxiliary name scope with\n",
      " |          the scope. If `False`, we don't touch name scope.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A scope that can be captured and reused.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: when trying to reuse within a create scope, or create within\n",
      " |          a reuse scope.\n",
      " |        TypeError: when the types of some arguments are not appropriate.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.variable_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AE_ts_model.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar 22 10:43:29 2016\n",
    "@author: Rob Romijnders\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "\n",
    "\n",
    "def open_data(direc, ratio_train=0.8, dataset=\"ECG5000\"):\n",
    "    \"\"\"Input:\n",
    "    direc: location of the UCR archive\n",
    "    ratio_train: ratio to split training and testset\n",
    "    dataset: name of the dataset in the UCR archive\"\"\"\n",
    "    datadir = direc + '/' + dataset + '/' + dataset\n",
    "    data_train = np.loadtxt(datadir + '_TRAIN', delimiter=',')\n",
    "    data_test_val = np.loadtxt(datadir + '_TEST', delimiter=',')[:-1]\n",
    "    data = np.concatenate((data_train, data_test_val), axis=0)\n",
    "\n",
    "    N, D = data.shape\n",
    "\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    ind = np.random.permutation(N)\n",
    "    return data[ind[:ind_cut], 1:], data[ind[ind_cut:], 1:], data[ind[:ind_cut], 0], data[ind[ind_cut:], 0]\n",
    "\n",
    "\n",
    "def plot_data(X_train, y_train, plot_row=5):\n",
    "    counts = dict(Counter(y_train))\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    f, axarr = plt.subplots(plot_row, num_classes)\n",
    "    for c in np.unique(y_train):  # Loops over classes, plot as columns\n",
    "        c = int(c)\n",
    "        ind = np.where(y_train == c)\n",
    "        ind_plot = np.random.choice(ind[0], size=plot_row)\n",
    "        for n in range(plot_row):  # Loops over rows\n",
    "            axarr[n, c].plot(X_train[ind_plot[n], :])\n",
    "            # Only shops axes for bottom row and left column\n",
    "            if n == 0: axarr[n, c].set_title('Class %.0f (%.0f)' % (c, counts[float(c)]))\n",
    "            if not n == plot_row - 1:\n",
    "                plt.setp([axarr[n, c].get_xticklabels()], visible=False)\n",
    "            if not c == 0:\n",
    "                plt.setp([axarr[n, c].get_yticklabels()], visible=False)\n",
    "    f.subplots_adjust(hspace=0)  # No horizontal space between subplots\n",
    "    f.subplots_adjust(wspace=0)  # No vertical space between subplots\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_z_run(z_run, label, ):\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    f1, ax1 = plt.subplots(2, 1)\n",
    "\n",
    "    PCA_model = TruncatedSVD(n_components=3).fit(z_run)\n",
    "    z_run_reduced = PCA_model.transform(z_run)\n",
    "    ax1[0].scatter(z_run_reduced[:, 0], z_run_reduced[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[0].set_title('PCA on z_run')\n",
    "    from sklearn.manifold import TSNE\n",
    "    tSNE_model = TSNE(verbose=2, perplexity=80, min_grad_norm=1E-12, n_iter=3000)\n",
    "    z_run_tsne = tSNE_model.fit_transform(z_run)\n",
    "    ax1[1].scatter(z_run_tsne[:, 0], z_run_tsne[:, 1], c=label, marker='*', linewidths=0)\n",
    "    ax1[1].set_title('tSNE on z_run')\n",
    "    return\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Hyperparameters\"\"\"\n",
    "        num_layers = config['num_layers']\n",
    "        hidden_size = config['hidden_size']\n",
    "        max_grad_norm = config['max_grad_norm']\n",
    "        batch_size = config['batch_size']\n",
    "        sl = config['sl']\n",
    "        crd = config['crd']\n",
    "        num_l = config['num_l']\n",
    "        learning_rate = config['learning_rate']\n",
    "        self.sl = sl\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Nodes for the input variables\n",
    "        self.x = tf.placeholder(\"float\", shape=[batch_size, sl], name='Input_data')\n",
    "        self.x_exp = tf.expand_dims(self.x, 1)\n",
    "        self.keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        with tf.variable_scope(\"Encoder\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            # Th encoder cell, multi-layered with dropout\n",
    "            cell_enc = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "            cell_enc = tf.contrib.rnn.DropoutWrapper(cell_enc, output_keep_prob=self.keep_prob)\n",
    "\n",
    "            # Initial state\n",
    "            initial_state_enc = cell_enc.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        # with tf.name_scope(\"Enc_2_lat\") as scope:\n",
    "            # layer for mean of z\n",
    "            W_mu = tf.get_variable('W_mu', [hidden_size, num_l])\n",
    "\n",
    "            outputs_enc, _ = tf.contrib.rnn.static_rnn(cell_enc,\n",
    "                                                      inputs=tf.unstack(self.x_exp, axis=2),\n",
    "                                                      initial_state=initial_state_enc)\n",
    "            cell_output = outputs_enc[-1]\n",
    "\n",
    "            b_mu = tf.get_variable('b_mu', [num_l])\n",
    "            self.z_mu = tf.nn.xw_plus_b(cell_output, W_mu, b_mu, name='z_mu')  # mu, mean, of latent space\n",
    "\n",
    "            # Train the point in latent space to have zero-mean and unit-variance on batch basis\n",
    "            lat_mean, lat_var = tf.nn.moments(self.z_mu, axes=[1])\n",
    "            self.loss_lat_batch = tf.reduce_mean(tf.square(lat_mean) + lat_var - tf.log(lat_var) - 1)\n",
    "\n",
    "        with tf.variable_scope(\"Lat_2_dec\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            # layer to generate initial state\n",
    "            W_state = tf.get_variable('W_state', [num_l, hidden_size])\n",
    "            b_state = tf.get_variable('b_state', [hidden_size])\n",
    "            z_state = tf.nn.xw_plus_b(self.z_mu, W_state, b_state, name='z_state')  # mu, mean, of latent space\n",
    "\n",
    "        with tf.variable_scope(\"Decoder\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            # The decoder, also multi-layered\n",
    "            cell_dec = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n",
    "\n",
    "            # Initial state\n",
    "            initial_state_dec = tuple([(z_state, z_state)] * num_layers)\n",
    "            dec_inputs = [tf.zeros([batch_size, 1])] * sl\n",
    "            # outputs_dec, _ = tf.nn.seq2seq.rnn_decoder(dec_inputs, initial_state_dec, cell_dec)\n",
    "            outputs_dec, _ = tf.contrib.rnn.static_rnn(cell_dec,\n",
    "                                                       inputs=dec_inputs,\n",
    "                                                       initial_state=initial_state_dec)\n",
    "        with tf.variable_scope(\"Out_layer\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            params_o = 2 * crd  # Number of coordinates + variances\n",
    "            W_o = tf.get_variable('W_o', [hidden_size, params_o])\n",
    "            b_o = tf.get_variable('b_o', [params_o])\n",
    "            outputs = tf.concat(outputs_dec, axis=0)  # tensor in [sl*batch_size,hidden_size]\n",
    "            h_out = tf.nn.xw_plus_b(outputs, W_o, b_o)\n",
    "            h_mu, h_sigma_log = tf.unstack(tf.reshape(h_out, [sl, batch_size, params_o]), axis=2)\n",
    "            h_sigma = tf.exp(h_sigma_log)\n",
    "            dist = tf.contrib.distributions.Normal(h_mu, h_sigma)\n",
    "            px = dist.log_prob(tf.transpose(self.x))\n",
    "            loss_seq = -px\n",
    "            self.loss_seq = tf.reduce_mean(loss_seq)\n",
    "\n",
    "        with tf.variable_scope(\"train\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            # Use learning rte decay\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            lr = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.1, staircase=False)\n",
    "\n",
    "            self.loss = self.loss_seq + self.loss_lat_batch\n",
    "\n",
    "            # Route the gradients so that we can plot them on Tensorboard\n",
    "            tvars = tf.trainable_variables()\n",
    "            # We clip the gradients to prevent explosion\n",
    "            grads = tf.gradients(self.loss, tvars)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n",
    "            self.numel = tf.constant([[0]])\n",
    "\n",
    "            # And apply the gradients\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "            gradients = zip(grads, tvars)\n",
    "            self.train_step = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "            #      for gradient, variable in gradients:  #plot the gradient of each trainable variable\n",
    "            #        if isinstance(gradient, ops.IndexedSlices):\n",
    "            #          grad_values = gradient.values\n",
    "            #        else:\n",
    "            #          grad_values = gradient\n",
    "            #\n",
    "            #        self.numel +=tf.reduce_sum(tf.size(variable))\n",
    "            #        tf.summary.histogram(variable.name, variable)\n",
    "            #        tf.summary.histogram(variable.name + \"/gradients\", grad_values)\n",
    "            #        tf.summary.histogram(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "\n",
    "            self.numel = tf.constant([[0]])\n",
    "        tf.summary.tensor_summary('lat_state', self.z_mu)\n",
    "        # Define one op to call all summaries\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        # and one op to initialize the variables\n",
    "        self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE_ts_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read data\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all labeled files\n",
    "path = os.path.expanduser(\"~/MultiPartyHRI/data/labeledData/\")\n",
    "fileInList = list(os.path.join(path+f) for f in os.listdir(path))\n",
    "data = pd.concat((pd.read_csv(f) for f in fileInList))\n",
    "\n",
    "# Import individual labeled files\n",
    "# data = pd.read_csv('./data/processedData/10-07-20-08.csv')\n",
    "# data = pd.read_csv('./data/processedData/12-07-18-08.csv')\n",
    "# data = pd.concat([data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "featureNames = list(data.columns)\n",
    "featureNames.remove('Activity')\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "data.dropna(axis=0, how='any', inplace= True)\n",
    "for i in range(1,len(featureNames)):\n",
    "    if featureNames[i] == 'timeStamp':\n",
    "        pass\n",
    "    else:\n",
    "        data[featureNames[i]] = feature_normalize(data[featureNames[i]]) \n",
    "        \n",
    "def timeStampToFloat(stringTime):\n",
    "    splitTime = stringTime.split(':')\n",
    "    hour = float(splitTime[0]) * 3600\n",
    "    minutes = float(splitTime[1]) * 60\n",
    "    return hour + minutes + float(splitTime[2])\n",
    "\n",
    "data['timeStamp'] = data['timeStamp'].apply(timeStampToFloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate X and Y\n",
    "X = data.iloc[:,:58]\n",
    "Y = data['Activity']\n",
    "# print(Y.unique())\n",
    "# Y.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.map( {'Approaching': 0, 'Interacting': 1, 'Leaving': 2, 'Uninterested': 3} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvlab/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "Y = Y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 12725 observations with 58 dimensions\n",
      "Train with approximately 5 epochs\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[ 8736  3368  6632 10835   264 10383 12060  7952 12022 10309 10181  8594\\n  3504  5463  2820  7605  9126  8039   661  4712  5651 12069  8318 10337\\n  9905  1555  7764  9217  5838 10656 11103  2861   589  3129  5022  6964\\n 11394 10987  2467  2461 10588  5181  9295  5384  4573 12668  8929 11410\\n  1657  9176   704  4541  9537 10909  5171 10413  8378  2576  8663 10767\\n 11732  5703 10305 11669] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-7b0d0a318c41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mbatch_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],\n\u001b[0;32m---> 66\u001b[0;31m                           feed_dict={model.x: X_train[batch_ind], model.keep_prob: dropout})\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplot_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[ 8736  3368  6632 10835   264 10383 12060  7952 12022 10309 10181  8594\\n  3504  5463  2820  7605  9126  8039   661  4712  5651 12069  8318 10337\\n  9905  1555  7764  9217  5838 10656 11103  2861   589  3129  5022  6964\\n 11394 10987  2467  2461 10588  5181  9295  5384  4573 12668  8929 11410\\n  1657  9176   704  4541  9537 10909  5171 10413  8378  2576  8663 10767\\n 11732  5703 10305 11669] not in index'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar 22 10:43:29 2016\n",
    "@author: Rob Romijnders\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "# from AE_ts_model import Model, open_data, plot_data, plot_z_run\n",
    "\n",
    "\"\"\"Hyperparameters\"\"\"\n",
    "# direc = '/home/rob/Dropbox/ml_projects/LSTM/UCR_TS_Archive_2015'\n",
    "LOG_DIR = \"/home/gvlab/Dropbox/.summaries\"\n",
    "config = {}  # Put all configuration information into the dict\n",
    "config['num_layers'] = 2  # number of layers of stacked RNN's\n",
    "config['hidden_size'] = 90  # memory cells in a layer\n",
    "config['max_grad_norm'] = 5  # maximum gradient norm during training\n",
    "config['batch_size'] = batch_size = 64\n",
    "config['learning_rate'] = .005\n",
    "config['crd'] = 1  # Hyperparameter for future generalization\n",
    "config['num_l'] = 20  # number of units in the latent space\n",
    "\n",
    "plot_every = 100  # after _plot_every_ GD steps, there's console output\n",
    "max_iterations = 1000  # maximum number of iterations\n",
    "dropout = 0.8  # Dropout rate\n",
    "\"\"\"Load the data\"\"\"\n",
    "# X_train, X_val, y_train, y_val = open_data('/home/rob/Dropbox/ml_projects/LSTM/UCR_TS_Archive_2015')\n",
    "\n",
    "N = X_train.shape[0]\n",
    "Nval = X_val.shape[0]\n",
    "D = X_train.shape[1]\n",
    "config['sl'] = sl = D  # sequence length\n",
    "print('We have %s observations with %s dimensions' % (N, D))\n",
    "\n",
    "# Organize the classes\n",
    "num_classes = len(np.unique(y_train))\n",
    "# base = np.min(y_train)  # Check if data is 0-based\n",
    "# if base != 0:\n",
    "#     y_train -= base\n",
    "#     y_val -= base\n",
    "\n",
    "# Plot data   # and save high quality plt.savefig('data_examples.eps', format='eps', dpi=1000)\n",
    "# plot_data(X_train, y_train)\n",
    "\n",
    "# Proclaim the epochs\n",
    "epochs = np.floor(batch_size * max_iterations / N)\n",
    "print('Train with approximately %d epochs' % epochs)\n",
    "\n",
    "\"\"\"Training time!\"\"\"\n",
    "model = Model(config)\n",
    "sess = tf.Session()\n",
    "perf_collect = np.zeros((2, int(np.floor(max_iterations / plot_every))))\n",
    "\n",
    "if True:\n",
    "    sess.run(model.init_op)\n",
    "    writer = tf.summary.FileWriter(LOG_DIR, sess.graph)  # writer for Tensorboard\n",
    "\n",
    "    step = 0  # Step is a counter for filling the numpy array perf_collect\n",
    "    for i in range(max_iterations):\n",
    "        batch_ind = np.random.choice(N, batch_size, replace=False)\n",
    "        result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],\n",
    "                          feed_dict={model.x: X_train[batch_ind], model.keep_prob: dropout})\n",
    "\n",
    "        if i % plot_every == 0:\n",
    "            # Save train performances\n",
    "            perf_collect[0, step] = loss_train = result[0]\n",
    "            loss_train_seq, lost_train_lat = result[1], result[2]\n",
    "\n",
    "            # Calculate and save validation performance\n",
    "            batch_ind_val = np.random.choice(Nval, batch_size, replace=False)\n",
    "\n",
    "            result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.merged],\n",
    "                              feed_dict={model.x: X_val[batch_ind_val], model.keep_prob: 1.0})\n",
    "            perf_collect[1, step] = loss_val = result[0]\n",
    "            loss_val_seq, lost_val_lat = result[1], result[2]\n",
    "            # and save to Tensorboard\n",
    "            summary_str = result[3]\n",
    "            writer.add_summary(summary_str, i)\n",
    "            writer.flush()\n",
    "\n",
    "            print(\"At %6s / %6s train (%5.3f, %5.3f, %5.3f), val (%5.3f, %5.3f,%5.3f) in order (total, seq, lat)\" % (\n",
    "            i, max_iterations, loss_train, loss_train_seq, lost_train_lat, loss_val, loss_val_seq, lost_val_lat))\n",
    "            step += 1\n",
    "if False:\n",
    "    ##Extract the latent space coordinates of the validation set\n",
    "    start = 0\n",
    "    label = []  # The label to save to visualize the latent space\n",
    "    z_run = []\n",
    "\n",
    "    while start + batch_size < Nval:\n",
    "        run_ind = range(start, start + batch_size)\n",
    "        z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: X_val[run_ind], model.keep_prob: 1.0})\n",
    "        z_run.append(z_mu_fetch)\n",
    "        start += batch_size\n",
    "\n",
    "    z_run = np.concatenate(z_run, axis=0)\n",
    "    label = y_val[:start]\n",
    "\n",
    "    plot_z_run(z_run, label)\n",
    "\n",
    "# Save the projections also to Tensorboard\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), step)\n",
    "config = projector.ProjectorConfig()\n",
    "# You can add multiple embeddings. Here we add only one.\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = model.z_mu.name\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "embedding.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "\n",
    "# Saves a configuration file that TensorBoard will read during startup.\n",
    "projector.visualize_embeddings(writer, config)\n",
    "saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), step + 1)\n",
    "writer.flush()\n",
    "\n",
    "\n",
    "\n",
    "# Now open Tensorboard with\n",
    "#  $tensorboard --logdir = LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
